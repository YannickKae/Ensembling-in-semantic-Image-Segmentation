{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1332611a",
   "metadata": {
    "papermill": {
     "duration": 0.00507,
     "end_time": "2023-07-27T11:57:30.016378",
     "exception": false,
     "start_time": "2023-07-27T11:57:30.011308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensemling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b7a752",
   "metadata": {},
   "source": [
    "The following cell is installing the necessary Python libraries needed for the notebook.\n",
    "\n",
    "1. `!pip install lightning`: This command installs the PyTorch Lightning library, which is a lightweight PyTorch wrapper for high-performance AI research. It simplifies the process of scaling and distributing models and provides high-level features for fast prototyping.\n",
    "\n",
    "2. `!pip install segmentation-models-pytorch`: This command installs the `segmentation_models.pytorch` library, which is a collection of PyTorch implementations of various image segmentation models (like U-Net, FPN, etc.) with pre-trained encoders. This library provides a simple and customizable interface for different segmentation models in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044c5b95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T11:57:30.035971Z",
     "iopub.status.busy": "2023-07-27T11:57:30.035300Z",
     "iopub.status.idle": "2023-07-27T11:58:02.587294Z",
     "shell.execute_reply": "2023-07-27T11:58:02.586065Z"
    },
    "papermill": {
     "duration": 32.560273,
     "end_time": "2023-07-27T11:58:02.589797",
     "exception": false,
     "start_time": "2023-07-27T11:57:30.029524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightning\r\n",
      "  Downloading lightning-2.0.6-py3-none-any.whl (1.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: Jinja2<5.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (3.1.2)\r\n",
      "Requirement already satisfied: PyYAML<8.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (5.4.1)\r\n",
      "Requirement already satisfied: arrow<3.0,>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.2.3)\r\n",
      "Requirement already satisfied: backoff<4.0,>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.2.1)\r\n",
      "Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.12.2)\r\n",
      "Requirement already satisfied: click<10.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (8.1.3)\r\n",
      "Collecting croniter<1.5.0,>=1.3.0 (from lightning)\r\n",
      "  Downloading croniter-1.4.1-py2.py3-none-any.whl (19 kB)\r\n",
      "Collecting dateutils<2.0 (from lightning)\r\n",
      "  Downloading dateutils-0.6.12-py2.py3-none-any.whl (5.7 kB)\r\n",
      "Collecting deepdiff<8.0,>=5.7.0 (from lightning)\r\n",
      "  Downloading deepdiff-6.3.1-py3-none-any.whl (70 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.7/70.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: fastapi<2.0,>=0.92.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.95.1)\r\n",
      "Requirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2023.5.0)\r\n",
      "Collecting inquirer<5.0,>=2.10.0 (from lightning)\r\n",
      "  Downloading inquirer-3.1.3-py3-none-any.whl (18 kB)\r\n",
      "Collecting lightning-cloud>=0.5.37 (from lightning)\r\n",
      "  Downloading lightning_cloud-0.5.37-py3-none-any.whl (596 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.7/596.7 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: lightning-utilities<2.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.8.0)\r\n",
      "Requirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.23.5)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\r\n",
      "Requirement already satisfied: psutil<7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (5.9.3)\r\n",
      "Requirement already satisfied: pydantic<2.1.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.10.7)\r\n",
      "Collecting python-multipart<2.0,>=0.0.5 (from lightning)\r\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: requests<4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.28.2)\r\n",
      "Requirement already satisfied: rich<15.0,>=12.3.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (12.6.0)\r\n",
      "Requirement already satisfied: starlette in /opt/conda/lib/python3.10/site-packages (from lightning) (0.26.1)\r\n",
      "Collecting starsessions<2.0,>=1.2.1 (from lightning)\r\n",
      "  Downloading starsessions-1.3.0-py3-none-any.whl (10 kB)\r\n",
      "Requirement already satisfied: torch<4.0,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.0.0)\r\n",
      "Requirement already satisfied: torchmetrics<2.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.11.4)\r\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.64.1)\r\n",
      "Requirement already satisfied: traitlets<7.0,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (5.9.0)\r\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.5.0)\r\n",
      "Requirement already satisfied: urllib3<4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.26.15)\r\n",
      "Requirement already satisfied: uvicorn<2.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.22.0)\r\n",
      "Requirement already satisfied: websocket-client<3.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.5.1)\r\n",
      "Requirement already satisfied: websockets<13.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (11.0.3)\r\n",
      "Requirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.0.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from arrow<3.0,>=1.2.0->lightning) (2.8.2)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4<6.0,>=4.8.0->lightning) (2.3.2.post1)\r\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from dateutils<2.0->lightning) (2023.3)\r\n",
      "Collecting ordered-set<4.2.0,>=4.0.2 (from deepdiff<8.0,>=5.7.0->lightning)\r\n",
      "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec<2025.0,>=2022.5.0->lightning) (3.8.4)\r\n",
      "Requirement already satisfied: blessed>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from inquirer<5.0,>=2.10.0->lightning) (1.20.0)\r\n",
      "Collecting python-editor>=1.0.4 (from inquirer<5.0,>=2.10.0->lightning)\r\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\r\n",
      "Collecting readchar>=3.0.6 (from inquirer<5.0,>=2.10.0->lightning)\r\n",
      "  Downloading readchar-4.0.5-py3-none-any.whl (8.5 kB)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<5.0->lightning) (2.1.2)\r\n",
      "Requirement already satisfied: pyjwt in /opt/conda/lib/python3.10/site-packages (from lightning-cloud>=0.5.37->lightning) (2.6.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from lightning-cloud>=0.5.37->lightning) (1.16.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->lightning) (3.0.9)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<4.0->lightning) (2.1.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<4.0->lightning) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<4.0->lightning) (2023.5.7)\r\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from rich<15.0,>=12.3.0->lightning) (0.9.1)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from rich<15.0,>=12.3.0->lightning) (2.15.1)\r\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette->lightning) (3.6.2)\r\n",
      "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from starsessions<2.0,>=1.2.1->lightning) (2.1.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (3.12.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (3.1)\r\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn<2.0->lightning) (0.14.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning) (23.1.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning) (4.0.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning) (1.9.1)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning) (1.3.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning) (1.3.1)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette->lightning) (1.3.0)\r\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /opt/conda/lib/python3.10/site-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning) (0.2.6)\r\n",
      "Requirement already satisfied: setuptools>=41.0 in /opt/conda/lib/python3.10/site-packages (from readchar>=3.0.6->inquirer<5.0,>=2.10.0->lightning) (59.8.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.11.0->lightning) (1.3.0)\r\n",
      "Installing collected packages: python-editor, readchar, python-multipart, ordered-set, inquirer, deepdiff, dateutils, croniter, starsessions, lightning-cloud, lightning\r\n",
      "Successfully installed croniter-1.4.1 dateutils-0.6.12 deepdiff-6.3.1 inquirer-3.1.3 lightning-2.0.6 lightning-cloud-0.5.37 ordered-set-4.1.0 python-editor-1.0.4 python-multipart-0.0.6 readchar-4.0.5 starsessions-1.3.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting segmentation-models-pytorch\r\n",
      "  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.15.1)\r\n",
      "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\r\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hCollecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\r\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: timm==0.9.2 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.9.2)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (4.64.1)\r\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (9.5.0)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.0.0)\r\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (3.0.0)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (5.4.1)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.14.1)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.3.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.23.5)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.28.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.12.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.5.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2023.5.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (21.3)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2.1.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2023.5.7)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.0.9)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\r\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\r\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=329c28123443d94e39fae6df549ed4220a535eda5d65c10bdd3f2e64da223ac5\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\r\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60966 sha256=6f73896cc8f72f027d4270d9aad6d51700050edda1fa7e15004255adee7676ee\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\r\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\r\n",
      "Installing collected packages: efficientnet-pytorch, pretrainedmodels, segmentation-models-pytorch\r\n",
      "Successfully installed efficientnet-pytorch-0.7.1 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.3\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install lightning\n",
    "!pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71370ab9",
   "metadata": {},
   "source": [
    "The next block of code is doing the following:\n",
    "\n",
    "1. Importing necessary modules: `torch`, `lightning` (presumably an alias for `pytorch_lightning`), `segmentation_models_pytorch` (for various image segmentation models), `CosineAnnealingLR` and `ReduceLROnPlateau` (for learning rate scheduling), `AdamW` (for optimization), `nn` (for neural network operations), `DataLoader` (for loading and batching data), `dice` (for performance evaluation), `sys` (for system-specific parameters and functions), `pandas` (for data manipulation and analysis), and `yaml` (for YAML file handling).\n",
    "\n",
    "2. Updating the System Path: The `sys.path.append` lines of code are used to add the directories where additional Python modules or packages are stored. Here, it includes paths for pre-trained models, efficientnet models, segmentation models, timm pre-trained models, and checkpoint files. Adding these paths to `sys.path` allows Python to find and import these modules/packages when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ca9a9d",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-07-27T11:58:02.608966Z",
     "iopub.status.busy": "2023-07-27T11:58:02.608610Z",
     "iopub.status.idle": "2023-07-27T11:58:20.182483Z",
     "shell.execute_reply": "2023-07-27T11:58:20.181534Z"
    },
    "papermill": {
     "duration": 17.586125,
     "end_time": "2023-07-27T11:58:20.184849",
     "exception": false,
     "start_time": "2023-07-27T11:58:02.598724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import lightning as l\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.functional import dice\n",
    "import sys\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "\n",
    "sys.path.append(\"../input/pretrained-models-pytorch\")\n",
    "sys.path.append(\"../input/efficientnet-pytorch\")\n",
    "sys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\n",
    "sys.path.append(\"/kaggle/input/timm-pretrained-resnest/resnest/\")\n",
    "sys.path.append(\"/kaggle/input/checkpoints-unet-resnest101e\")\n",
    "sys.path.append(\"/kaggle/input/checkpoint-deeplabv3plus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2009f95",
   "metadata": {},
   "source": [
    "The next cell defines a PyTorch Dataset class, named `ContrailsDataset`, which is utilized to preprocess and provide data for the model training in a structured format.\n",
    "\n",
    "1. The `__init__` method initializes the dataset object with a dataframe, `df`, which contains the data information. It also sets an image size and a flag `train` to differentiate between train and validation datasets. The normalization parameters for images are also defined here. If the image size is not 256, a resizing transform is created.\n",
    "\n",
    "2. The `__getitem__` method defines how to fetch one piece of data (an image-label pair). It fetches a row from the dataframe based on an index, reads the image file from the path specified in the row, separates the image and label data, converts them into PyTorch tensors, reshapes and reorders the dimensions of the image tensor, resizes the image if necessary, normalizes the image, and then returns the image and label.\n",
    "\n",
    "3. The `__len__` method returns the total number of items in the dataset (which is the length of the dataframe).\n",
    "\n",
    "In essence, this class provides a custom way of accessing and processing the Contrails dataset, tailored to work well with PyTorch's DataLoader and the other components of the training framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7ed9931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T11:58:23.283888Z",
     "iopub.status.busy": "2023-07-27T11:58:23.283550Z",
     "iopub.status.idle": "2023-07-27T11:58:23.294866Z",
     "shell.execute_reply": "2023-07-27T11:58:23.293991Z"
    },
    "papermill": {
     "duration": 0.023357,
     "end_time": "2023-07-27T11:58:23.296862",
     "exception": false,
     "start_time": "2023-07-27T11:58:23.273505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class ContrailsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, image_size=256, train=True):\n",
    "\n",
    "        self.df = df\n",
    "        self.trn = train\n",
    "        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        self.image_size = image_size\n",
    "        if image_size != 256:\n",
    "            self.resize_image = T.transforms.Resize(image_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        con_path = row.path\n",
    "        con = np.load(str(con_path))\n",
    "\n",
    "        img = con[..., :-1]\n",
    "        label = con[..., -1]\n",
    "\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n",
    "\n",
    "        if self.image_size != 256:\n",
    "            img = self.resize_image(img)\n",
    "\n",
    "        img = self.normalize_image(img)\n",
    "\n",
    "        return img.float(), label.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458313c1",
   "metadata": {
    "papermill": {
     "duration": 0.008448,
     "end_time": "2023-07-27T11:58:23.313969",
     "exception": false,
     "start_time": "2023-07-27T11:58:23.305521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ensemble Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a1007",
   "metadata": {
    "papermill": {
     "duration": 0.008463,
     "end_time": "2023-07-27T11:58:23.331529",
     "exception": false,
     "start_time": "2023-07-27T11:58:23.323066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model One"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c79586b",
   "metadata": {},
   "source": [
    "This first model code defines a custom PyTorch Lightning Module called `LightningModule_Unet_resnest_101`, which encapsulates a U-Net model with ResNeSt101 encoder and its training logic. The U-Net model is a commonly used architecture for image segmentation tasks.\n",
    "\n",
    "1. In the `__init__` method, the U-Net model is defined with \"timm-resnest101e\" as the encoder, without using pretrained weights. The model takes 3-channel images as input and outputs a single-channel mask. The activation function is not specified. The Dice loss function is selected for computing the binary segmentation loss with a smoothing factor of 1.0. Two lists, `val_step_outputs` and `val_step_labels`, are also initialized for storing predictions and labels during validation, respectively.\n",
    "\n",
    "2. The `forward` method passes an input batch through the model and returns the model's output.\n",
    "\n",
    "3. In the `validation_step` method, the model's forward pass is called on the input images, the output is resized to 256x256 if necessary, the loss is computed by comparing predictions with the ground truth labels, and the loss is logged. The predictions and labels are stored for further use at the end of the validation epoch.\n",
    "\n",
    "4. `on_validation_epoch_end` method calculates the Dice score, a common metric for image segmentation tasks, using the predictions and labels accumulated during the validation steps. The calculated Dice score is logged. If the current process is the main one in distributed computing settings (i.e., `self.trainer.global_rank == 0`), the current epoch number is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c981e55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T11:58:23.350838Z",
     "iopub.status.busy": "2023-07-27T11:58:23.349943Z",
     "iopub.status.idle": "2023-07-27T11:58:23.360849Z",
     "shell.execute_reply": "2023-07-27T11:58:23.360005Z"
    },
    "papermill": {
     "duration": 0.022655,
     "end_time": "2023-07-27T11:58:23.362771",
     "exception": false,
     "start_time": "2023-07-27T11:58:23.340116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lightning as l\n",
    "\n",
    "class LightningModule_Unet_resnest_101(l.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = smp.Unet(encoder_name=\"timm-resnest101e\",\n",
    "                              encoder_weights=None,\n",
    "                              in_channels=3,\n",
    "                              classes=1,\n",
    "                              activation=None,\n",
    "                              )\n",
    "        self.loss_module = smp.losses.DiceLoss(mode=\"binary\", smooth=1.0)\n",
    "        self.val_step_outputs = []\n",
    "        self.val_step_labels = []\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.model(batch)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        preds = torch.nn.functional.interpolate(preds, size=256, mode='bilinear')\n",
    "        loss = self.loss_module(preds, labels)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.val_step_outputs.append(preds)\n",
    "        self.val_step_labels.append(labels)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        all_preds = torch.cat(self.val_step_outputs)\n",
    "        all_labels = torch.cat(self.val_step_labels)\n",
    "        all_preds = torch.sigmoid(all_preds)\n",
    "        self.val_step_outputs.clear()\n",
    "        self.val_step_labels.clear()\n",
    "        val_dice = dice(all_preds, all_labels.long())\n",
    "        self.log(\"val_dice\", val_dice, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        if self.trainer.global_rank == 0:\n",
    "            print(f\"\\nEpoch: {self.current_epoch}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59771f48",
   "metadata": {
    "papermill": {
     "duration": 0.00847,
     "end_time": "2023-07-27T11:58:23.379869",
     "exception": false,
     "start_time": "2023-07-27T11:58:23.371399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Model two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0894926",
   "metadata": {},
   "source": [
    "The second model code defines a PyTorch Lightning Module called `LightningModule_DeepLabV3Plus`, which encapsulates the DeepLabV3+ model for semantic image segmentation.\n",
    "\n",
    "1. In the `__init__` method, the DeepLabV3+ model is defined with \"tu-resnest26d\" as the encoder, without pretrained weights. The model takes images with three channels as input and outputs a one-channel mask. No activation function is specified. The Dice loss function, which is a common choice for binary segmentation tasks, is chosen as the loss function with a smoothing factor of 1.0. Two lists, `val_step_outputs` and `val_step_labels`, are also initialized for storing predictions and labels during validation, respectively.\n",
    "\n",
    "2. The `forward` method returns the output from passing the input batch through the model.\n",
    "\n",
    "3. In the `validation_step` method, a forward pass of the model is performed on the input images, the output is interpolated to a size of 256x256 if needed, and the loss is calculated by comparing predictions with the ground truth labels. This loss is logged. The model's predictions and labels are stored for further use at the end of the validation epoch.\n",
    "\n",
    "4. The `on_validation_epoch_end` method concatenates all the predictions and labels collected during validation, computes the sigmoid function on the predictions to convert them into probabilities, and then computes the Dice score, a popular metric for segmentation tasks. The calculated Dice score is logged. If the current process is the main one in distributed computing settings (i.e., `self.trainer.global_rank == 0`), the current epoch number is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce8d0f0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T11:58:23.398992Z",
     "iopub.status.busy": "2023-07-27T11:58:23.398223Z",
     "iopub.status.idle": "2023-07-27T11:58:23.408735Z",
     "shell.execute_reply": "2023-07-27T11:58:23.407866Z"
    },
    "papermill": {
     "duration": 0.022362,
     "end_time": "2023-07-27T11:58:23.410808",
     "exception": false,
     "start_time": "2023-07-27T11:58:23.388446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LightningModule_DeepLabV3Plus(l.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = smp.DeepLabV3Plus(encoder_name=\"tu-resnest26d\", \n",
    "                              encoder_weights=None,\n",
    "                              in_channels=3,\n",
    "                              classes=1,\n",
    "                              activation=None,\n",
    "                              )\n",
    "\n",
    "        self.loss_module = smp.losses.DiceLoss(mode=\"binary\", smooth=1.0)\n",
    "        self.val_step_outputs = []\n",
    "        self.val_step_labels = []\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.model(batch)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        preds = torch.nn.functional.interpolate(preds, size=256, mode='bilinear')\n",
    "        loss = self.loss_module(preds, labels)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.val_step_outputs.append(preds)\n",
    "        self.val_step_labels.append(labels)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        all_preds = torch.cat(self.val_step_outputs)\n",
    "        all_labels = torch.cat(self.val_step_labels)\n",
    "        all_preds = torch.sigmoid(all_preds)\n",
    "        self.val_step_outputs.clear()\n",
    "        self.val_step_labels.clear()\n",
    "        val_dice = dice(all_preds, all_labels.long())\n",
    "        self.log(\"val_dice\", val_dice, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        if self.trainer.global_rank == 0:\n",
    "            print(f\"\\nEpoch: {self.current_epoch}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c10f01",
   "metadata": {
    "papermill": {
     "duration": 0.008433,
     "end_time": "2023-07-27T11:58:23.428099",
     "exception": false,
     "start_time": "2023-07-27T11:58:23.419666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1250b92c",
   "metadata": {
    "papermill": {
     "duration": 0.008365,
     "end_time": "2023-07-27T11:58:23.444983",
     "exception": false,
     "start_time": "2023-07-27T11:58:23.436618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### From https://github.com/Lightning-AI/lightning/discussions/7249 averaging/ stacking with n pretrained models    \n",
    "\n",
    "Can be used to implement simple weighted average predictions/weighted average ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471cb427",
   "metadata": {},
   "source": [
    "The next code defines a PyTorch Lightning Module named `MyEnsemble`, which encapsulates an ensemble model composed of two previously trained models.\n",
    "\n",
    "1. In the `__init__` method, the two models and their corresponding weights are passed in. These models are assigned to `self.modelA` and `self.modelB`, and the weights are assigned to `self.weight_model_one` and `self.weight_model_two`. Both models are frozen to prevent changes during further training. The Dice loss function is selected as the loss function for this ensemble model with a smoothing factor of 1.0. The hyperparameters (except for the two models) are saved. Two lists, `val_step_outputs` and `val_step_labels`, are also initialized for storing predictions and labels during validation, respectively.\n",
    "\n",
    "2. The `configure_optimizers` method defines the Adam optimizer with a learning rate of 1e-3 to be used for training the model.\n",
    "\n",
    "3. The `forward` method takes an input `x`, performs a forward pass through both models (`modelA` and `modelB`), multiplies the respective outputs with the model weights, and sums the results to generate the final output.\n",
    "\n",
    "4. In the `validation_step` method, the ensemble model's forward method is used on the input images, the output is interpolated to a size of 256x256 if needed, and the loss is calculated by comparing the predictions with the ground truth labels. This loss is logged. The ensemble model's predictions and the corresponding labels are stored for use at the end of the validation epoch.\n",
    "\n",
    "5. The `on_validation_epoch_end` method concatenates all the predictions and labels collected during the validation, applies the sigmoid function on the predictions to convert them into probabilities, and calculates the Dice score, a common metric for segmentation tasks. The calculated Dice score is logged. If the current process is the main one in distributed computing settings (i.e., `self.trainer.global_rank == 0`), the current epoch number is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "813236e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T11:58:23.464137Z",
     "iopub.status.busy": "2023-07-27T11:58:23.463319Z",
     "iopub.status.idle": "2023-07-27T11:58:23.475716Z",
     "shell.execute_reply": "2023-07-27T11:58:23.474833Z"
    },
    "papermill": {
     "duration": 0.023917,
     "end_time": "2023-07-27T11:58:23.477672",
     "exception": false,
     "start_time": "2023-07-27T11:58:23.453755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyEnsemble(l.LightningModule):\n",
    "    def __init__(self, model_one, model_two, weight_model_one, weight_model_two):\n",
    "        super(MyEnsemble, self).__init__()\n",
    "        self.modelA = model_one\n",
    "        self.modelB = model_two\n",
    "        self.weight_model_two = weight_model_two\n",
    "        self.weight_model_one = weight_model_one\n",
    "        self.modelA.freeze()\n",
    "        self.modelB.freeze()\n",
    "        self.loss_module = smp.losses.DiceLoss(mode=\"binary\", smooth=1.0)\n",
    "        self.save_hyperparameters(ignore=['model_one','model_two'])\n",
    "        self.val_step_outputs = []\n",
    "        self.val_step_labels = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.modelA(x)\n",
    "        x2 = self.modelB(x)\n",
    "        result = x1*self.weight_model_one + x2*self.weight_model_two\n",
    "        return result\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.forward(imgs)\n",
    "        preds = torch.nn.functional.interpolate(preds, size=256, mode='bilinear')\n",
    "        loss = self.loss_module(preds, labels)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.val_step_outputs.append(preds)\n",
    "        self.val_step_labels.append(labels)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        all_preds = torch.cat(self.val_step_outputs)\n",
    "        all_labels = torch.cat(self.val_step_labels)\n",
    "        all_preds = torch.sigmoid(all_preds)\n",
    "        self.val_step_outputs.clear()\n",
    "        self.val_step_labels.clear()\n",
    "        val_dice = dice(all_preds, all_labels.long())\n",
    "        self.log(\"val_dice\", val_dice, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        if self.trainer.global_rank == 0:\n",
    "            print(f\"\\nEpoch: {self.current_epoch}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574b124c",
   "metadata": {},
   "source": [
    "This last block first loads the two pre-trained models, the U-net model with a ResNeSt-101 encoder (`model_resnest_101`), and the DeepLabV3Plus model (`Deeplab`), from saved checkpoints. \n",
    "\n",
    "It then combines these two models into an ensemble model (`model`) using the `MyEnsemble` class. The weights for the ensemble model are set as equal (0.5 each), meaning that the ensemble's output is the average of the output of the two models.\n",
    "\n",
    "Next, it prepares the validation dataset:\n",
    "\n",
    "1. It reads the validation dataframe from a CSV file and appends the directory path to the record IDs to form the complete path for each .npy file.\n",
    "\n",
    "2. This dataframe is then used to instantiate an object of the `ContrailsDataset` class, which creates a PyTorch dataset for validation. The image size is set to 384 and the `train` flag is set to `False`, indicating this is for validation/testing, not for training.\n",
    "\n",
    "3. A DataLoader is then created for the validation dataset, with a batch size of 40. The DataLoader is set not to shuffle the data (since this is for validation, not training), and to use 2 worker processes for data loading.\n",
    "\n",
    "Finally, a PyTorch Lightning `Trainer` is created, and the `validate` method is called on it with the ensemble model and the validation dataloader, performing validation on the ensemble model with the validation dataset. The validation results will include the logged metrics (e.g., validation loss and Dice score) printed to the console or saved in the Lightning logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6419f370",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T11:58:23.497607Z",
     "iopub.status.busy": "2023-07-27T11:58:23.496822Z",
     "iopub.status.idle": "2023-07-27T11:59:36.828764Z",
     "shell.execute_reply": "2023-07-27T11:59:36.827708Z"
    },
    "papermill": {
     "duration": 73.344541,
     "end_time": "2023-07-27T11:59:36.830947",
     "exception": false,
     "start_time": "2023-07-27T11:58:23.486406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "WARNING: Missing logger folder: /kaggle/working/lightning_logs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e387405e284e8bbb9e05d71c1d5141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_dice          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4649423360824585     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5468075275421143     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_dice         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4649423360824585    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5468075275421143    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.5468075275421143, 'val_dice': 0.4649423360824585}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_resnest_101 = LightningModule_Unet_resnest_101().load_from_checkpoint(\"/kaggle/input/checkpoints-unet-resnest101e/model.ckpt\")\n",
    "Deeplab = LightningModule_DeepLabV3Plus().load_from_checkpoint(\"/kaggle/input/checkpoint-deeplabv3plus/model (1).ckpt\")\n",
    "\n",
    "model = MyEnsemble(model_resnest_101, Deeplab, 0.5, 0.5)\n",
    "\n",
    "valid_df = pd.read_csv(\"/kaggle/input/contrails-images-ash-color/valid_df.csv\")\n",
    "valid_df[\"path\"] = \"/kaggle/input/contrails-images-ash-color/contrails/\" + valid_df[\"record_id\"].astype(str) + \".npy\"\n",
    "\n",
    "dataset_validation = ContrailsDataset(valid_df, 384, train=False)\n",
    "data_loader_validation = DataLoader(\n",
    "    dataset_validation,\n",
    "    batch_size=40,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "trainer = l.Trainer()\n",
    "# trainer.validate(model_resnest_101, dataloaders=data_loader_validation)\n",
    "trainer.validate(model, dataloaders=data_loader_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbc9650",
   "metadata": {
    "papermill": {
     "duration": 0.010002,
     "end_time": "2023-07-27T11:59:36.851273",
     "exception": false,
     "start_time": "2023-07-27T11:59:36.841271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5674da",
   "metadata": {
    "papermill": {
     "duration": 0.00993,
     "end_time": "2023-07-27T11:59:36.871072",
     "exception": false,
     "start_time": "2023-07-27T11:59:36.861142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ideas\n",
    "\n",
    "Try adding/weigthing models before softmax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 140.783006,
   "end_time": "2023-07-27T11:59:40.130932",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-27T11:57:19.347926",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03e387405e284e8bbb9e05d71c1d5141": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8d871000ac40458bba4361fb621901d2",
        "IPY_MODEL_cf1b50166bba4c5e83634865f6830c84",
        "IPY_MODEL_26c51a6c5a7246b0b14b4885e8618643"
       ],
       "layout": "IPY_MODEL_9785b12db15f4213b2b8a45daecbd184"
      }
     },
     "26c51a6c5a7246b0b14b4885e8618643": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e124547a20bc44d1917d2bf6f601728b",
       "placeholder": "​",
       "style": "IPY_MODEL_72c1fb98735646b48b810c0d4fca1acb",
       "value": " 47/47 [00:57&lt;00:00,  1.23s/it]"
      }
     },
     "53fa418ae264424e9d53248cff96652a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "72c1fb98735646b48b810c0d4fca1acb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "85747986629444aaa9203296aa6e1d8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "889db024be324a19b0daedd1c5e605a0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8d871000ac40458bba4361fb621901d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_889db024be324a19b0daedd1c5e605a0",
       "placeholder": "​",
       "style": "IPY_MODEL_85747986629444aaa9203296aa6e1d8f",
       "value": "Validation DataLoader 0: 100%"
      }
     },
     "9785b12db15f4213b2b8a45daecbd184": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "cf1b50166bba4c5e83634865f6830c84": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d52dec70bc2247f0b52c291ac3e1e1ff",
       "max": 47,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_53fa418ae264424e9d53248cff96652a",
       "value": 47
      }
     },
     "d52dec70bc2247f0b52c291ac3e1e1ff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e124547a20bc44d1917d2bf6f601728b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
